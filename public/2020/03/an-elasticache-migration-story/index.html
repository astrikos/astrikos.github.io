<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>An ElastiCache Migration Story - Andreas Strikos</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="An ElastiCache Migration Story"><meta itemprop=description content="How the Beat way of working allowed us to scale a production component in almost two weeks."><meta itemprop=datePublished content="2020-03-17T00:00:00+00:00"><meta itemprop=dateModified content="2020-03-17T00:00:00+00:00"><meta itemprop=wordCount content="1741"><meta itemprop=keywords content="AWS,Redis,ElastiCache,"><meta property="og:title" content="An ElastiCache Migration Story"><meta property="og:description" content="How the Beat way of working allowed us to scale a production component in almost two weeks."><meta property="og:type" content="article"><meta property="og:url" content="https://astrikos.github.io/2020/03/an-elasticache-migration-story/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-03-17T00:00:00+00:00"><meta property="article:modified_time" content="2020-03-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="An ElastiCache Migration Story"><meta name=twitter:description content="How the Beat way of working allowed us to scale a production component in almost two weeks."><link rel=stylesheet type=text/css media=screen href=https://astrikos.github.iocss/normalize.css><link rel=stylesheet type=text/css media=screen href=https://astrikos.github.iocss/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://astrikos.github.iocss/dark.css><script src=https://astrikos.github.iojs/feather.min.js></script>
<script src=https://astrikos.github.iojs/main.js></script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://astrikos.github.io><img src=/boy-student-sitting.webp alt="Andreas Strikos"></a></div><h1 class=site-title><a href=https://astrikos.github.io>Andreas Strikos</a></h1><div class=site-description><p>Software and Infrastructure Engineer</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/astrikos title=Github><i data-feather=github></i></a></li><li><a href=https://www.linkedin.com/in/astrikos/ title=LinkedIn><i data-feather=linkedin></i></a></li><span class=scheme-toggle><a href=# id=scheme-toggle></a></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>17</span>
<span class=rest>Mar 2020</span></div></div><div class=matter><h1 class=title>An ElastiCache Migration Story</h1></div></div><div class=markdown><figure><img src=/FCpSUmj7zPDy7yzVKpav2Q.jpeg></figure><h3 id=the-problem><strong>The Problem</strong></h3><p>At Beat, a tech company that has been around for a while, our product consists of a bigger, constantly decreasing in size, monolith and a constantly increasing number of microservices. For our services that need persistence, we use several kinds of databases to store their state and, at the same time, we have chosen Redis to be leveraged as a second-layer cache in front of them. So far we have been using the AWS ElastiCache managed service in a disabled cluster mode. While this architecture served us quite well for some time, it recently caused us some headaches and it soon became obvious that we could no longer scale any further with this setup.</p><figure><img src=/pI_X_m1zkG3g8d-k.png><figcaption>Cluster Disabled AWS ElastiCache architecture</figcaption></figure><p>From the above graph, it’s easy to identify that the master node is a bottleneck and the only way to scale up is vertically. We tried to do this a couple of times but it was painful and a cause for downtimes. Furthermore, scaling vertically was increasing our AWS bills by a fair amount without allowing us to use the full potential of these instances. Even adding a single node to the cluster was very time consuming and sometimes was causing periods of small or even big downtimes.</p><p>Adding to all the above, our monolith service is prone to creating hotkeys that can cause spikes of load on specific events. We knew this was not ideal and was on our radar to refactor it but this would take some time. In the meantime we wanted to have a system that could scale much better.</p><p>In order to tackle the problem and prevent bigger downtimes in the near future, we decided to assemble a subteam consisting of backend, QA, and infrastructure engineers that would be on this full-time in order to come up with an alternative scalable solution. After several internal whiteboard sessions, some reassuring talks with our AWS technical account manager and support engineers, and of course generous portions of coffee, we decided to go with a cluster mode enabled architecture. In theory, this would allow us to scale and load balance traffic from our heavy-loaded masters. The new architecture would look like:</p><figure><img src=/RVIW24IgxlUm7B6C.png><figcaption>Cluster Enabled AWS ElastiCache architecture</figcaption></figure><h3 id=stress-testing-the-newsolution><strong>Stress testing the new solution</strong></h3><p>Before investing multiple people’s time and energy on the new architecture, we wanted to see if it would meet our expectations and cover our needs for growth. Our main specifications were the following:</p><ul><li>If a specific shard node is overloaded, we should be able to add replica node(s) without any impact to the existing cluster.</li><li>If a specific shard is getting significantly more load than others, we should be able to create a new shard and rebalance our cluster without any downtime or impact to our clients.</li><li>If we want to scale a cluster vertically and change our instance types, it should be without downtime.</li></ul><p>For each of these tests we created a test cluster that we loaded with some dummy data and we started making queries from multiple clients.</p><p>We used tools like <a href=https://github.com/RedisLabs/memtier_benchmark>memtier</a> and <a href=https://github.com/yueyoum/redis-benchmark>redis-benchmark</a>, as well as some home grown scripts that allowed us to make our testbed as close to our production as possible and also allowed us to test our use cases.</p><p>When our tests showed us that we had the green light to use this architecture, we moved to the next phase which was capacity planning for the new cluster.</p><h3 id=capacity-planning><strong>Capacity Planning</strong></h3><p>After the stress tests showed us that the new setup was promising and had all the needed checkmarks, we went through our current system and calculated the resources we needed to serve the current load at the time. The goal was for the initial version of the new setup to be able to withhold double that load. After that, any scaling should be easy to happen.</p><p>The fact that Redis server is single threaded allowed us to focus on metrics like memory, network bandwidth, and number of connections for the whole cluster.</p><p>We aimed to plan our capacity on the conservative side for a couple of reasons. We could add more shards and replica nodes later easily without any downtime and we also knew that, from the client’s point of view, we had some improvements in the pipeline that would help reduce our cluster load.</p><h3 id=migration-time><strong>Migration Time</strong></h3><p>Once we were ready for our new cluster and the codebase to support it, we came up with a release plan that consisted of multiple phases. We wanted each phase to introduce as few changes as it could at the same time within our system. This way, it would be easier to observe the impact of each change in a far easier and clear manner.</p><h4 id=phase-1-or-the-testing-the-watersphase><strong>Phase 1 (or the &ldquo;Testing the waters&rdquo; phase)</strong></h4><p>For the first phase, we didn’t make any radical changes on our backend besides the fact that we should support Redis cluster mode, and we enabled the cluster on a smaller portion of our users, initially in the Greek market. After our quality assurance engineers gave us the green light, we made the switch. The results, however, were not very impressive.</p><figure><img src=/e5-mjqFm0DxNl5IcapH_0A.png><figcaption>Cluster CPU utilization after enabling cluster mode</figcaption></figure><p>The graphs above showed us that our new cluster was somehow load balancing our traffic in a better way and the load was split between the multiple masters. However, the traffic distribution was not as expected, so we continued digging in and making our traffic better distributed across our nodes.</p><h4 id=phase-2-or-the-weve-just-getting-warmed-upphase><strong>Phase 2 (or the &ldquo;We’ve just getting warmed up&rdquo; phase)</strong></h4><p>We did have, however, a hidden ace up our sleeve; we were suspecting that our new clients were not using persistent connections. Using the <a href=https://github.com/iovisor/bcc/blob/master/tools/tcpconnect.py><em>tcpconnect</em></a> script from <a href=https://github.com/iovisor/bcc><em>bcc</em> <em>tools</em></a>, we observed a huge rate of new connections towards the Redis cluster’s TCP port.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-vim data-lang=vim><span style=display:flex><span>astrikos@co<span style=color:#099>-247</span>-api<span style=color:#099>-100</span>:~$ sudo <span style=color:#009926>/usr/</span>share<span style=color:#009926>/bcc/</span>tools/tcpconnect -P <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>PID COMM IP SADDR DADDR DPORT  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>28083</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>3</span>.<span style=color:#099>61</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>23983</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>3</span>.<span style=color:#099>251</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>17563</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>2</span>.<span style=color:#099>214</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>21281</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>2</span>.<span style=color:#099>248</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>757</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>2</span>.<span style=color:#099>214</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>13566</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>2</span>.<span style=color:#099>138</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>4982</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>2</span>.<span style=color:#099>27</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>1084</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>3</span>.<span style=color:#099>120</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>21281</span> php-fpm7.<span style=color:#099>1</span> <span style=color:#099>4</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>0</span>.<span style=color:#099>92</span> <span style=color:#099>10</span>.<span style=color:#099>9</span>.<span style=color:#099>3</span>.<span style=color:#099>219</span> <span style=color:#099>6379</span>  <span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><p>With every Redis command, we were creating a new connection to the server. That, of course, was very expensive for the ElastiCache nodes on system level, since it was causing the Linux kernel to work a lot on opening and closing these new connections. After we deployed our code with the new persistent connection flag, we were delighted to see the following impact.</p><figure><img src=/O6Z7481cKZMcV9Al0gh33g.jpeg><figcaption>Left: Cluster Current Connections — Right: Cluster New Connections</figcaption></figure><figure><img src=/XrSZ6uxnebKXN2b9567shQ.png><figcaption>Cluster CPU utilization after enabling persistent connections</figcaption></figure><p>As you can observe, CPU went drastically down, current connections increased, since now they were long lived, and new connections decreased to almost 0. Also rerunning the <em>tcpconnect</em> tool showed us a dramatically decreased rate of new connections within our instances.</p><h4 id=phase-3-or-the-finding-needles-in-a-haystack-phase><strong>Phase 3 (or the &ldquo;Finding needles in a haystack&rdquo; phase)</strong></h4><p>We still, however, hadn’t solved our problem of specific shard masters not being balanced loaded. We knew that our Redis traffic pattern was a ratio of 1:7 write/read commands, which meant that the master nodes shouldn’t be that loaded if traffic were split between master and replicas. It was time for some network inspection to see what kind of traffic we exchange with the different Redis cluster nodes. After firing a tcpdump in one of our instances that was running clients to our cluster, we noticed something that was interesting:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-vim data-lang=vim><span style=display:flex><span><span style=color:#099>20</span>:<span style=color:#099>54</span>:<span style=color:#099>36</span>.<span style=color:#099>071016</span> IP <span style=color:#099>10</span>.<span style=color:#099>3</span>.<span style=color:#099>2</span>.<span style=color:#099>202</span>.<span style=color:#099>52244</span> &gt; <span style=color:#099>10</span>.<span style=color:#099>3</span>.<span style=color:#099>2</span>.<span style=color:#099>246</span>.<span style=color:#099>6379</span>: Flags [P.], seq <span style=color:#099>119034</span>:<span style=color:#099>119078</span>, ack <span style=color:#099>77591</span>, win <span style=color:#099>852</span>, options [nop,nop,TS val <span style=color:#099>9057048</span> ecr <span style=color:#099>2717873315</span>], length <span style=color:#099>25</span>: RESP “GET” “core\_settings”<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span><span style=color:#099>20</span>:<span style=color:#099>54</span>:<span style=color:#099>36</span>.<span style=color:#099>081016</span> IP <span style=color:#099>10</span>.<span style=color:#099>3</span>.<span style=color:#099>2</span>.<span style=color:#099>246</span>.<span style=color:#099>6379</span> &gt; <span style=color:#099>10</span>.<span style=color:#099>3</span>.<span style=color:#099>2</span>.<span style=color:#099>202</span>.<span style=color:#099>52244</span>: Flags [P.], seq <span style=color:#099>119000</span>:<span style=color:#099>119034</span>, ack <span style=color:#099>119078</span>, win <span style=color:#099>227</span>, options [nop,nop,TS val <span style=color:#099>3670031817</span> ecr <span style=color:#099>10018445</span>], length <span style=color:#099>29</span>: RESP “MOVED <span style=color:#099>13782</span> <span style=color:#099>10</span>.<span style=color:#099>3</span>.<span style=color:#099>2</span>.<span style=color:#099>35</span>:<span style=color:#099>6379</span>”<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><p>Our client instance is the machine with IP 10.3.2.202 and the Redis replica node is the IP 10.3.2.246.</p><p>We knew from the cluster sharding map that the specific Redis replica was part of the shard that was responsible for the key we asked for. The response we got, though was a <em>MOVED</em> response that was redirecting us to another instance with IP 10.3.2.35 that happened to be our master node for this shard. After some research, we discovered that in order for our replicas to respond to <a href=https://redis.io/commands/readonly><em>READONLY</em></a> commands, we would have to prepend a <em>READONLY</em> prefix in front of our commands. Our backend engineers prepared the changes in our codebase and as soon as we deployed the new changes, we saw the following:</p><figure><img src=/K9F8STpj9mUxTQpfpa1_jw.png><figcaption>Cluster CPU utilization changes</figcaption></figure><p>That had done the trick, the gap between the master and the replica nodes had significantly closed down.</p><h4 id=phase-4-or-the-finishing-touchesphase><strong>Phase 4 (or the &ldquo;Finishing touches&rdquo; phase)</strong></h4><p>Something in the graphs, however, still didn’t allow us to justify that cold beer that comes after a good win. If you look carefully at the above graph, our master nodes were no longer getting as much traffic as we would have expected. What seemed to have happened, was that we moved our traffic from our masters to our replicas, resulting in a non-homogeneous traffic pattern. Talking with our backend engineers, this proved to be a feature of our internal library, which was developed as part of our previous setup. Since we didn’t need it anymore we disabled it and allowed master nodes to get a piece of the read only queries as well. After this final touch, we had the following satisfying results.</p><figure><img src=/9OCcaWd50DlJ5jm-shTXmQ.png><figcaption>Cluster CPU utilization after READONLY changes</figcaption></figure><h4 id=phase-5-or-the-celebrate-phase><strong>Phase 5 (or the “Celebrate” Phase)</strong></h4><p>It was finally time to celebrate and enjoy that cold beer we had been dreaming of for the last couple of weeks. The graph below was finally what we were expecting and hoping for.</p><h3 id=conclusion><strong>Conclusion</strong></h3><p>All Beat Engineers are proud of working together as one team. We managed to make this happen in just over two weeks because <strong>multiple teams and engineers from different domains, expertises and crafts, joined forces and worked in harmony</strong>. From our infrastructure engineers preparing the clusters, to the backend engineers hacking in our codebase and libraries, and our quality engineers making sure everything worked as expected before we went live each time, everyone worked together and, above all, had fun.</p><hr><blockquote><p>This post was originally published in Beat Engineering Blog, as part of my engineering work at Beat. Cross-posting here for posterity reasons.</p><p><a href=https://medium.com/@anstrikos/an-elasticache-migration-story-9090a524b3f8>An ElastiCache Migration Story</a></p><p>It also lead to an AWS case study that can be read in <a href=https://aws.amazon.com/solutions/case-studies/beat-elasticache-case-study/>here</a>.</p></blockquote></div><div class=tags><ul class=flat><li class=tag-li><a href=/tags/aws>AWS</a></li><li class=tag-li><a href=/tags/redis>Redis</a></li><li class=tag-li><a href=/tags/elasticache>ElastiCache</a></li></ul></div><div class=back><a href=https://astrikos.github.io><span aria-hidden=true>← Back</span></a></div><div class=back></div></div></div><div class="footer wrapper"><nav class=nav><div>2020</div></nav></div><script>feather.replace()</script></body></html>