<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Andreas Strikos</title>
    <link>https://astrikos.github.io/tags/aws/</link>
    <description>Recent content in AWS on Andreas Strikos</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://astrikos.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A curious case of AWS NLB timeouts in Kubernetes</title>
      <link>https://astrikos.github.io/2021/04/a-curious-case-of-aws-nlb-timeouts-in-kubernetes/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://astrikos.github.io/2021/04/a-curious-case-of-aws-nlb-timeouts-in-kubernetes/</guid>
      <description>Some time ago Beat&amp;rsquo;s infrastructure team underwent a migration of the flagship service into our Kubernetes clusters and at the time we documented our preparations in a previous article.
Prior to the migration of the monolith we already had several services inside our Kubernetes cluster, which the monolith and other external services had to reach. The way to do that, was to use ingress controllers with help from external-dns to automatically expose microservices to a predefined DNS schema endpoint.</description>
    </item>
    
    <item>
      <title>Yet Another Kubernetes DNS Latency Story</title>
      <link>https://astrikos.github.io/2020/10/yet-another-kubernetes-dns-latency-story/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://astrikos.github.io/2020/10/yet-another-kubernetes-dns-latency-story/</guid>
      <description>In previous Beat articles we talked about the importance of DNS in our Kubernetes clusters and how we managed to scale through different techniques. We also mentioned that one of the upcoming versions of Kubernetes would provide the ultimate way to handle our DNS problems and give us some peace of mind while working on other parts. At Beat, we use kops to provision and maintain our cluster so we opted on waiting until kops would support it.</description>
    </item>
    
    <item>
      <title>An ElastiCache Migration Story</title>
      <link>https://astrikos.github.io/2020/03/an-elasticache-migration-story/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://astrikos.github.io/2020/03/an-elasticache-migration-story/</guid>
      <description>The Problem At Beat, a tech company that has been around for a while, our product consists of a bigger, constantly decreasing in size, monolith and a constantly increasing number of microservices. For our services that need persistence, we use several kinds of databases to store their state and, at the same time, we have chosen Redis to be leveraged as a second-layer cache in front of them. So far we have been using the AWS ElastiCache managed service in a disabled cluster mode.</description>
    </item>
    
  </channel>
</rss>
